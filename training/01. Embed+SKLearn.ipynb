{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embed+SKLearn",
      "provenance": [],
      "collapsed_sections": [
        "fQmDS-zBe11X",
        "IwQtiKVsdxDR",
        "mn4JdEPDkezj",
        "rcepf55Bq5tl",
        "jo10Ig6J0Mb9",
        "h_VyvfL30y9x",
        "CXuLSooG2i1B"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH2leGvhSdYv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6fcaddd9-1727-4527-ba82-7c7950a46dcb"
      },
      "source": [
        "%cd '/content/drive/My Drive/Toxic Content Detection'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Toxic Content Detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQmDS-zBe11X"
      },
      "source": [
        "## Loading Dataset\n",
        "\n",
        "Dataset taken from: [Kaggle - Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)\n",
        "\n",
        "Dataset has 8 columns:\n",
        "*   Index: **id** \n",
        "*   Input: **comment_text**\n",
        "*   Target Classes: **toxic**, **severe_toxic**, **obscene**, **thread**, **insult**, **identity_hate**\n",
        "\n",
        "We combined all the target classes into a single target class named **toxic** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMmOIzlNSpdv"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbgNuAA3S-Jt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "fd33ea0f-87dc-44f6-ce5f-0f1e118ff4a3"
      },
      "source": [
        "# Loading train data\n",
        "\n",
        "train = pd.read_csv('Datasets/jigsaw-toxic-comment/train.csv', index_col=\"id\")\n",
        "train['toxic'] = (train['toxic'] | train['severe_toxic'] | train['obscene'] | train['threat'] | train['insult'] | train['identity_hate']).astype('category')\n",
        "train = train.drop(['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1)\n",
        "\n",
        "train.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 159571 entries, 0000997932d777bf to fff46fc426af1f9a\n",
            "Data columns (total 2 columns):\n",
            " #   Column        Non-Null Count   Dtype   \n",
            "---  ------        --------------   -----   \n",
            " 0   comment_text  159571 non-null  object  \n",
            " 1   toxic         159571 non-null  category\n",
            "dtypes: category(1), object(1)\n",
            "memory usage: 2.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e24EDIG-jklw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "df834b7c-50e5-4358-f6d9-4191d884a286"
      },
      "source": [
        "train.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0000997932d777bf</th>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000103f0d9cfb60f</th>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000113f07ec002fd</th>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0001b41b1c6bb37e</th>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0001d958c54c6e35</th>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>00025465d4725e87</th>\n",
              "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0002bcb3da6cb337</th>\n",
              "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>00031b1e95af7921</th>\n",
              "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>00037261f536c51d</th>\n",
              "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>00040093b2687caa</th>\n",
              "      <td>alignment on this subject and which are contra...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       comment_text toxic\n",
              "id                                                                       \n",
              "0000997932d777bf  Explanation\\nWhy the edits made under my usern...     0\n",
              "000103f0d9cfb60f  D'aww! He matches this background colour I'm s...     0\n",
              "000113f07ec002fd  Hey man, I'm really not trying to edit war. It...     0\n",
              "0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...     0\n",
              "0001d958c54c6e35  You, sir, are my hero. Any chance you remember...     0\n",
              "00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...     0\n",
              "0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK     1\n",
              "00031b1e95af7921  Your vandalism to the Matt Shirvington article...     0\n",
              "00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...     0\n",
              "00040093b2687caa  alignment on this subject and which are contra...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqWmchV3UvGY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "b34868ca-c271-4b2e-9c51-3deade0b0ada"
      },
      "source": [
        "# Loading test data\n",
        "\n",
        "test = pd.read_csv('Datasets/jigsaw-toxic-comment/test.csv', index_col=\"id\")\n",
        "test_labels = pd.read_csv('Datasets/jigsaw-toxic-comment/test_labels.csv', index_col=\"id\")\n",
        "test = test.join(test_labels)\n",
        "test['toxic'] = (test['toxic'] | test['severe_toxic'] | test['obscene'] | test['threat'] | test['insult'] | test['identity_hate']).astype('category')\n",
        "test = test.drop(['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1)\n",
        "test = test.drop(test[test['toxic'] == -1].index)\n",
        "test['toxic'] = test['toxic'].cat.remove_unused_categories()\n",
        "\n",
        "test.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 63978 entries, 0001ea8717f6de06 to fffb5451268fb5ba\n",
            "Data columns (total 2 columns):\n",
            " #   Column        Non-Null Count  Dtype   \n",
            "---  ------        --------------  -----   \n",
            " 0   comment_text  63978 non-null  object  \n",
            " 1   toxic         63978 non-null  category\n",
            "dtypes: category(1), object(1)\n",
            "memory usage: 1.0+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xIWVYIVii2z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "bc38ec22-936c-4e13-cfb1-9ea8a1b0740d"
      },
      "source": [
        "test.tail(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>fff8f64043129fa2</th>\n",
              "      <td>:Jerome, I see you never got around to this…! ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fff9d70fe0722906</th>\n",
              "      <td>==Lucky bastard== \\n http://wikimediafoundatio...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fffa8a11c4378854</th>\n",
              "      <td>==shame on you all!!!== \\n\\n You want to speak...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fffac2a094c8e0e2</th>\n",
              "      <td>MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fffb5451268fb5ba</th>\n",
              "      <td>\" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       comment_text toxic\n",
              "id                                                                       \n",
              "fff8f64043129fa2  :Jerome, I see you never got around to this…! ...     0\n",
              "fff9d70fe0722906  ==Lucky bastard== \\n http://wikimediafoundatio...     0\n",
              "fffa8a11c4378854  ==shame on you all!!!== \\n\\n You want to speak...     0\n",
              "fffac2a094c8e0e2  MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...     1\n",
              "fffb5451268fb5ba  \" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwQtiKVsdxDR"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "Since the comments are raw, they contain **formatting markups** and **special characters**.\n",
        "\n",
        "### We do the following preprocessing:\n",
        "*   Removing all the punctuations. \n",
        "*   Splitting words by alphabets (this removes non-alphanumeric values).\n",
        "*   Stripping the words to remove extra spacing.\n",
        "*   Rejoining the cleaned words.\n",
        "*   Lower casing all the comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDcC9kM5fW0U"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = re.split('\\W+', text)\n",
        "    text = \" \".join(tokens).strip().lower()\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpytDmwHiC8g"
      },
      "source": [
        "# Applying preprocessing on train and test data\n",
        "\n",
        "train['comment_text'] = train['comment_text'].apply(preprocess)\n",
        "test['comment_text'] = test['comment_text'].apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z7cwjY3lnKs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "a60ddb88-e5c2-4702-833a-55ad210b141f"
      },
      "source": [
        "train.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0000997932d777bf</th>\n",
              "      <td>explanation why the edits made under my userna...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000103f0d9cfb60f</th>\n",
              "      <td>daww he matches this background colour im seem...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000113f07ec002fd</th>\n",
              "      <td>hey man im really not trying to edit war its j...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0001b41b1c6bb37e</th>\n",
              "      <td>more i cant make any real suggestions on impro...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0001d958c54c6e35</th>\n",
              "      <td>you sir are my hero any chance you remember wh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>00025465d4725e87</th>\n",
              "      <td>congratulations from me as well use the tools ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0002bcb3da6cb337</th>\n",
              "      <td>cocksucker before you piss around on my work</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>00031b1e95af7921</th>\n",
              "      <td>your vandalism to the matt shirvington article...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>00037261f536c51d</th>\n",
              "      <td>sorry if the word nonsense was offensive to yo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>00040093b2687caa</th>\n",
              "      <td>alignment on this subject and which are contra...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       comment_text toxic\n",
              "id                                                                       \n",
              "0000997932d777bf  explanation why the edits made under my userna...     0\n",
              "000103f0d9cfb60f  daww he matches this background colour im seem...     0\n",
              "000113f07ec002fd  hey man im really not trying to edit war its j...     0\n",
              "0001b41b1c6bb37e  more i cant make any real suggestions on impro...     0\n",
              "0001d958c54c6e35  you sir are my hero any chance you remember wh...     0\n",
              "00025465d4725e87  congratulations from me as well use the tools ...     0\n",
              "0002bcb3da6cb337       cocksucker before you piss around on my work     1\n",
              "00031b1e95af7921  your vandalism to the matt shirvington article...     0\n",
              "00037261f536c51d  sorry if the word nonsense was offensive to yo...     0\n",
              "00040093b2687caa  alignment on this subject and which are contra...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuPYn9ddlpMg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "e79fbdde-74ef-4e57-bd7b-9893482545c8"
      },
      "source": [
        "test.tail(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>fff8f64043129fa2</th>\n",
              "      <td>jerome i see you never got around to this i m ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fff9d70fe0722906</th>\n",
              "      <td>lucky bastard httpwikimediafoundationorgwikipr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fffa8a11c4378854</th>\n",
              "      <td>shame on you all you want to speak about gays ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fffac2a094c8e0e2</th>\n",
              "      <td>mel gibson is a nazi bitch who makes shitty mo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fffb5451268fb5ba</th>\n",
              "      <td>unicorn lair discovery supposedly a unicorn la...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       comment_text toxic\n",
              "id                                                                       \n",
              "fff8f64043129fa2  jerome i see you never got around to this i m ...     0\n",
              "fff9d70fe0722906  lucky bastard httpwikimediafoundationorgwikipr...     0\n",
              "fffa8a11c4378854  shame on you all you want to speak about gays ...     0\n",
              "fffac2a094c8e0e2  mel gibson is a nazi bitch who makes shitty mo...     1\n",
              "fffb5451268fb5ba  unicorn lair discovery supposedly a unicorn la...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn4JdEPDkezj"
      },
      "source": [
        "## Sampling dataset\n",
        "\n",
        "Here target class distribution is imbalanced ( 9 non-toxic : 1 toxic ), we used different sampling methods to balance the data:\n",
        "\n",
        "*   **Original**: Uses imbalanced classes for training.\n",
        "*   **Undersampled**: Randomly selects subset of samples from classes with higher counts to match the count of class with lowest count.\n",
        "*   **Oversampled**: Randomly selects samples from classes with lower counts to duplicate to match the count of class with highest count.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvsTFwlHkt3R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "58b0f01f-6877-4f6b-97a5-dd0ca3674392"
      },
      "source": [
        "# Looking at the distribution of target class\n",
        "train['toxic'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    143346\n",
              "1     16225\n",
              "Name: toxic, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4-T2RJiZU1t"
      },
      "source": [
        "sampling=\"Original\"\n",
        "\n",
        "if sampling==\"Original\":\n",
        "    x_train_raw = train['comment_text'].to_numpy().reshape((-1))\n",
        "    y_train = train['toxic'].to_numpy().reshape((-1))\n",
        "elif sampling==\"Undersampled\":\n",
        "    from imblearn.under_sampling import RandomUnderSampler\n",
        "    undersampler = RandomUnderSampler()\n",
        "    x_train_raw, y_train = undersampler.fit_resample(\n",
        "                                train['comment_text'].to_numpy().reshape((-1, 1)), \n",
        "                                train['toxic'].to_numpy().reshape((-1, 1))\n",
        "                            )\n",
        "elif sampling==\"Oversampled\":\n",
        "    from imblearn.over_sampling import RandomOverSampler\n",
        "    oversampler = RandomOverSampler()\n",
        "    x_train_raw, y_train = oversampler.fit_resample(\n",
        "                               train['comment_text'].to_numpy().reshape((-1, 1)), \n",
        "                               train['toxic'].to_numpy().reshape((-1, 1))\n",
        "                            )\n",
        "\n",
        "x_test_raw = test['comment_text'].to_numpy().reshape((-1))\n",
        "y_test = test['toxic'].to_numpy().reshape((-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcepf55Bq5tl"
      },
      "source": [
        "## Embedding Layer\n",
        "\n",
        "We used various pretrained embedding layers from [tfhub](https://tfhub.dev/) and compared their performances.\n",
        "\n",
        "### We used following embedding layers:\n",
        "*   [Wiki Words 500](https://tfhub.dev/google/Wiki-words-500/2): based on skipgram version of word2vec with 1 out-of-vocabulary bucket.\n",
        "*   [NNLM](https://tfhub.dev/google/nnlm-en-dim128/2): based on feed-forward Neural-Net Language Models with 3 hidden layers.\n",
        "*   [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4): based on deep averaging network (DAN) encoder.\n",
        "*   [Universal Sentence Encoder Lite](https://tfhub.dev/google/universal-sentence-encoder-lite/2): based on Transformer architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YogM6Yydyn7l"
      },
      "source": [
        "!pip3 install --quiet sentencepiece\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# For NNLM, WW500, USE\n",
        "def hub_embed(link):    \n",
        "    def load_embed():\n",
        "        import tensorflow as tf\n",
        "\n",
        "        embed = hub.load(link)\n",
        "        def get_embedding(msgs):\n",
        "            return embed(msgs).numpy()\n",
        "        return get_embedding\n",
        "    return load_embed\n",
        "\n",
        "# For USE lite\n",
        "def use_lite_embed():\n",
        "    import sentencepiece as spm\n",
        "    from absl import logging\n",
        "    import tensorflow.compat.v1 as tf\n",
        "    tf.disable_v2_behavior()\n",
        "\n",
        "    module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\")\n",
        "    input_placeholder = tf.sparse_placeholder(tf.int64, shape=[None, None])\n",
        "    encodings = module(\n",
        "        inputs=dict(\n",
        "            values=input_placeholder.values,\n",
        "            indices=input_placeholder.indices,\n",
        "            dense_shape=input_placeholder.dense_shape))\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        spm_path = sess.run(module(signature=\"spm_path\"))\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.Load(spm_path)\n",
        "\n",
        "    def process_to_IDs_in_sparse_format(sp, sentences):\n",
        "        ids = [sp.EncodeAsIds(x) for x in sentences]\n",
        "        max_len = max(len(x) for x in ids)\n",
        "        dense_shape=(len(ids), max_len)\n",
        "        values=[item for sublist in ids for item in sublist]\n",
        "        indices=[[row,col] for row in range(len(ids)) for col in range(len(ids[row]))]\n",
        "        return (values, indices, dense_shape)\n",
        "\n",
        "    def embed(msgs):\n",
        "        values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, msgs)\n",
        "        logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "        with tf.Session() as session:\n",
        "            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "            return session.run(\n",
        "                encodings,\n",
        "                feed_dict={input_placeholder.values: values,\n",
        "                            input_placeholder.indices: indices,\n",
        "                            input_placeholder.dense_shape: dense_shape})\n",
        "    return embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZBNOiIMaD7T"
      },
      "source": [
        "# Embeddings\n",
        "embeddings = [\n",
        "              {'code':\"WW500\", 'name': \"wiki-word-500\", 'embed_func': hub_embed(\"https://tfhub.dev/google/Wiki-words-500-with-normalization/2\")},\n",
        "              {'code':\"NNLM128\", 'name': \"nnlm-en-dim128\", 'embed_func': hub_embed(\"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\")},\n",
        "              {'code':\"USE\", 'name': \"universal-sentence-encoder\", 'embed_func': hub_embed(\"https://tfhub.dev/google/universal-sentence-encoder/4\")},\n",
        "              {'code':\"USEL\", 'name': \"universal-sentence-encoder-lite\", 'embed_func': use_lite_embed},\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo10Ig6J0Mb9"
      },
      "source": [
        "## Classifiers\n",
        "\n",
        "16 different classifiers from SKLearn were used to compare their performances, training times and prediction times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwKUkN0EmDdM"
      },
      "source": [
        "# Classifiers\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, PassiveAggressiveClassifier, Perceptron\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
        "\n",
        "classifiers = [\n",
        "               {\"name\": \"LR\", \"classifier\": LogisticRegression},\n",
        "               {\"name\": \"SGD\", \"classifier\": SGDClassifier},\n",
        "               {\"name\": \"Ridge\", \"classifier\": RidgeClassifier},\n",
        "               {\"name\": \"LDA\", \"classifier\": LinearDiscriminantAnalysis},\n",
        "               {\"name\": \"PA\", \"classifier\": PassiveAggressiveClassifier},\n",
        "               {\"name\": \"GNB\", \"classifier\": GaussianNB},\n",
        "               {\"name\": \"Perceptron\", \"classifier\": Perceptron},\n",
        "               {\"name\": \"ET\", \"classifier\": ExtraTreeClassifier},\n",
        "               {\"name\": \"DT\", \"classifier\": DecisionTreeClassifier},\n",
        "               {\"name\": \"RF\", \"classifier\": RandomForestClassifier},\n",
        "               {\"name\": \"AdaB\", \"classifier\": AdaBoostClassifier},\n",
        "               {\"name\": \"MLP\", \"classifier\": MLPClassifier},\n",
        "               {\"name\": \"Bagging\", \"classifier\": BaggingClassifier},\n",
        "               {\"name\": \"GradB\", \"classifier\": GradientBoostingClassifier},\n",
        "               {\"name\": \"SVC\", \"classifier\": SVC},\n",
        "               {\"name\": \"KNN\", \"classifier\": KNeighborsClassifier},\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_VyvfL30y9x"
      },
      "source": [
        "## Metrics\n",
        "\n",
        "All the models are evaluated on 6 different metrics as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjXhVaF7wf4D"
      },
      "source": [
        "# Metrics\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "def confusion_matrix_proper(y_true, pred):\n",
        "    return \",\".join(str(score) for score in confusion_matrix(y_true, pred).ravel())\n",
        "\n",
        "metrics = [\n",
        "           {\"name\": \"accuracy\", \"metric\": accuracy_score},\n",
        "           {\"name\": \"balanced_accuracy\", \"metric\": balanced_accuracy_score},\n",
        "           {\"name\": \"tn,fp,fn,tp\", \"metric\": confusion_matrix_proper},\n",
        "           {\"name\": \"precision\", \"metric\": precision_score},\n",
        "           {\"name\": \"recall\", \"metric\": recall_score},\n",
        "           {\"name\": \"f1\", \"metric\": f1_score},\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXuLSooG2i1B"
      },
      "source": [
        "## Training and Evaluating Models\n",
        "\n",
        "We saved all the combinations of **models** and **embeddings** in different folders based on sampling method used.\n",
        "\n",
        "These folders contain a csv file comparing performance of different models using different embeddings and all the saved models.\n",
        "\n",
        "Links to saved models and results:\n",
        "*   [Original](https://bit.ly/2yyOczB)\n",
        "*   [Undersampled](https://bit.ly/3eBgbzc)\n",
        "*   [Oversampled](https://bit.ly/3eDikug)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwzUhxeSAche"
      },
      "source": [
        "# Helper function to apply embedding to batches of data instead of whole data at a time\n",
        "\n",
        "def batch_map(func, iterable, batch_size=1):\n",
        "    from tqdm import tqdm\n",
        "    l = len(iterable)\n",
        "    result = list()\n",
        "    for ndx in tqdm(range(0, l, batch_size)):\n",
        "        result.extend(func(iterable[ndx:min(ndx + batch_size, l)]))\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfqwQtBc3S16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d9ae5ee-0b2f-47dc-fe4c-701106ef2294"
      },
      "source": [
        "from time import time\n",
        "import pickle\n",
        "\n",
        "with open(f'Models/Embed+SKLearn/{sampling}/models.csv', 'w') as logfile:\n",
        "    logfile.write(f\"timestamp,classifier,embedding,time_to_train,time_to_predict\")\n",
        "    for metric in metrics:\n",
        "        logfile.write(f\",{metric['name']}\")\n",
        "    logfile.write(\"\\n\")\n",
        "\n",
        "for embedding in embeddings:\n",
        "    print(f\"Embedding: {embedding['name']}\\n\")\n",
        "    \n",
        "    # Embedding\n",
        "    print(f\"Processing embedding...\", end=\"\")\n",
        "    embed = embedding['embed_func']()\n",
        "    print(\"Done\")\n",
        "    print(\"Embedding train set...\", end=\"\")\n",
        "    x_train = batch_map(embed, x_train_raw.reshape((-1)), batch_size=10000)\n",
        "    print(\"Done\")\n",
        "\n",
        "    print(\"Embedding test set...\", end=\"\")\n",
        "    x_test = batch_map(embed, x_test_raw.reshape((-1)), batch_size=10000)\n",
        "    print(\"Done\")\n",
        "\n",
        "    for classifier in classifiers:\n",
        "        # Initialize classifier\n",
        "        print(f\"\\nClassifier: {classifier['name']}\\n\")\n",
        "        clf = classifier['classifier']()\n",
        "\n",
        "        # Train with timing\n",
        "        print(\"Training...\", end=\"\")\n",
        "        time_to_train = time()\n",
        "        clf.fit(x_train, y_train)\n",
        "        time_to_train = time() - time_to_train\n",
        "        print(f\"Done. Took {time_to_train}s\")\n",
        "\n",
        "        # Save model\n",
        "        print(\"Saving model...\", end=\"\")\n",
        "        with open(f\"Models/Embed+SKLearn/Original/{embedding['code']}-{classifier['name']}.model\", 'wb') as clf_file:\n",
        "            pickle.dump(clf, clf_file)\n",
        "        print(\"Done\")\n",
        "\n",
        "        # Predict with timing\n",
        "        print(\"Predicting for test set...\", end=\"\")\n",
        "        time_to_predict = time()\n",
        "        prediction = clf.predict(x_test)\n",
        "        time_to_predict = time() - time_to_predict\n",
        "        print(f\"Done. Took {time_to_predict}s\")\n",
        "\n",
        "        # Save logs\n",
        "        print(f\"Metrics:\")\n",
        "        with open('Models/Embed+SKLearn/Original/models.csv', 'a') as logfile:\n",
        "            logfile.write(f\"{time()},{classifier['name']},{embedding['name']},{time_to_train},{time_to_predict}\")\n",
        "            for metric in metrics:\n",
        "                score = metric['metric'](y_test, prediction)\n",
        "                print(f\"{metric['name']}: {score}\")\n",
        "                logfile.write(f\",{score}\")\n",
        "            logfile.write(\"\\n\")\n",
        "\n",
        "        del clf\n",
        "        del prediction\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "    del embed\n",
        "    del x_train\n",
        "    del x_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding: wiki-word-500\n",
            "\n",
            "Processing embedding..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Embedding train set..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:03<00:00,  4.19it/s]\n",
            "  0%|          | 0/7 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Embedding test set..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [00:01<00:00,  4.86it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "\n",
            "Classifier: LR\n",
            "\n",
            "Training...Done. Took 10.429743766784668s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.20615601539611816s\n",
            "Metrics:\n",
            "accuracy: 0.9203319891212605\n",
            "balanced_accuracy: 0.7902137876885089\n",
            "tn,fp,fn,tp: 54957,2778,2319,3924\n",
            "precision: 0.585496866606983\n",
            "recall: 0.628543969245555\n",
            "f1: 0.6062572421784472\n",
            "\n",
            "Classifier: SGD\n",
            "\n",
            "Training...Done. Took 2.8988256454467773s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.18036818504333496s\n",
            "Metrics:\n",
            "accuracy: 0.9260370752446153\n",
            "balanced_accuracy: 0.7605172383872743\n",
            "tn,fp,fn,tp: 55782,1953,2779,3464\n",
            "precision: 0.6394683404098209\n",
            "recall: 0.5548614448181963\n",
            "f1: 0.5941680960548885\n",
            "\n",
            "Classifier: Ridge\n",
            "\n",
            "Training...Done. Took 1.51019287109375s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.14039850234985352s\n",
            "Metrics:\n",
            "accuracy: 0.9094845103004158\n",
            "balanced_accuracy: 0.5674152109338817\n",
            "tn,fp,fn,tp: 57298,437,5354,889\n",
            "precision: 0.6704374057315233\n",
            "recall: 0.14239948742591702\n",
            "f1: 0.23490553573787817\n",
            "\n",
            "Classifier: LDA\n",
            "\n",
            "Training...Done. Took 16.66868495941162s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.20540308952331543s\n",
            "Metrics:\n",
            "accuracy: 0.9093438369439495\n",
            "balanced_accuracy: 0.6699099478815598\n",
            "tn,fp,fn,tp: 55853,1882,3918,2325\n",
            "precision: 0.5526503446636558\n",
            "recall: 0.37241710716001925\n",
            "f1: 0.44497607655502397\n",
            "\n",
            "Classifier: PA\n",
            "\n",
            "Training...Done. Took 3.1374406814575195s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.17990326881408691s\n",
            "Metrics:\n",
            "accuracy: 0.8951983494326174\n",
            "balanced_accuracy: 0.8315030516693636\n",
            "tn,fp,fn,tp: 52576,5159,1546,4697\n",
            "precision: 0.4765625\n",
            "recall: 0.7523626461637033\n",
            "f1: 0.5835145040064601\n",
            "\n",
            "Classifier: GNB\n",
            "\n",
            "Training...Done. Took 0.8071401119232178s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.518803596496582s\n",
            "Metrics:\n",
            "accuracy: 0.7843164837913033\n",
            "balanced_accuracy: 0.6887090459995371\n",
            "tn,fp,fn,tp: 46621,11114,2685,3558\n",
            "precision: 0.24250272628135225\n",
            "recall: 0.5699183085055262\n",
            "f1: 0.340234281616065\n",
            "\n",
            "Classifier: Perceptron\n",
            "\n",
            "Training...Done. Took 2.798391342163086s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.17976713180541992s\n",
            "Metrics:\n",
            "accuracy: 0.8866172746881741\n",
            "balanced_accuracy: 0.8488202676839578\n",
            "tn,fp,fn,tp: 51718,6017,1237,5006\n",
            "precision: 0.45414134083280416\n",
            "recall: 0.8018580810507768\n",
            "f1: 0.5798679485694428\n",
            "\n",
            "Classifier: ET\n",
            "\n",
            "Training...Done. Took 1.8381526470184326s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.12853264808654785s\n",
            "Metrics:\n",
            "accuracy: 0.8601706836725125\n",
            "balanced_accuracy: 0.6668793569452502\n",
            "tn,fp,fn,tp: 52368,5367,3579,2664\n",
            "precision: 0.33171460590212926\n",
            "recall: 0.42671792407496395\n",
            "f1: 0.37326607818411095\n",
            "\n",
            "Classifier: DT\n",
            "\n",
            "Training...Done. Took 327.449426651001s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.12904644012451172s\n",
            "Metrics:\n",
            "accuracy: 0.8624527181218544\n",
            "balanced_accuracy: 0.6975012554461101\n",
            "tn,fp,fn,tp: 52103,5632,3168,3075\n",
            "precision: 0.35316412082232684\n",
            "recall: 0.4925516578567996\n",
            "f1: 0.41137123745819393\n",
            "\n",
            "Classifier: RF\n",
            "\n",
            "Training...Done. Took 616.7826714515686s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 2.591357946395874s\n",
            "Metrics:\n",
            "accuracy: 0.9203788802400825\n",
            "balanced_accuracy: 0.682524169340381\n",
            "tn,fp,fn,tp: 56468,1267,3827,2416\n",
            "precision: 0.6559869671463481\n",
            "recall: 0.38699343264456193\n",
            "f1: 0.4868023372959903\n",
            "\n",
            "\n",
            "\n",
            "Embedding: nnlm-en-dim128\n",
            "\n",
            "Processing embedding..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|▋         | 1/16 [00:00<00:02,  5.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Embedding train set..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:02<00:00,  6.03it/s]\n",
            " 14%|█▍        | 1/7 [00:00<00:00,  6.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Embedding test set..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [00:01<00:00,  6.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "\n",
            "Classifier: LR\n",
            "\n",
            "Training...Done. Took 3.1329996585845947s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.08158206939697266s\n",
            "Metrics:\n",
            "accuracy: 0.9166119603613743\n",
            "balanced_accuracy: 0.7580094215784083\n",
            "tn,fp,fn,tp: 55141,2594,2741,3502\n",
            "precision: 0.574475065616798\n",
            "recall: 0.5609482620534999\n",
            "f1: 0.5676310884188346\n",
            "\n",
            "Classifier: SGD\n",
            "\n",
            "Training...Done. Took 0.811194658279419s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.05281496047973633s\n",
            "Metrics:\n",
            "accuracy: 0.9226765450623652\n",
            "balanced_accuracy: 0.7277977637890265\n",
            "tn,fp,fn,tp: 55999,1736,3211,3032\n",
            "precision: 0.6359060402684564\n",
            "recall: 0.4856639436168509\n",
            "f1: 0.5507220052674598\n",
            "\n",
            "Classifier: Ridge\n",
            "\n",
            "Training...Done. Took 0.3179759979248047s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.06239771842956543s\n",
            "Metrics:\n",
            "accuracy: 0.9075150833098877\n",
            "balanced_accuracy: 0.5338236221294272\n",
            "tn,fp,fn,tp: 57627,108,5809,434\n",
            "precision: 0.8007380073800738\n",
            "recall: 0.06951786000320359\n",
            "f1: 0.12792925571112748\n",
            "\n",
            "Classifier: LDA\n",
            "\n",
            "Training...Done. Took 4.3846917152404785s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.07563138008117676s\n",
            "Metrics:\n",
            "accuracy: 0.9162368314107975\n",
            "balanced_accuracy: 0.6234427942512033\n",
            "tn,fp,fn,tp: 56998,737,4622,1621\n",
            "precision: 0.6874469889737065\n",
            "recall: 0.2596508089059747\n",
            "f1: 0.37693291477735147\n",
            "\n",
            "Classifier: PA\n",
            "\n",
            "Training...Done. Took 0.9464199542999268s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.05340933799743652s\n",
            "Metrics:\n",
            "accuracy: 0.9199255994248022\n",
            "balanced_accuracy: 0.6341295776861147\n",
            "tn,fp,fn,tp: 57113,622,4501,1742\n",
            "precision: 0.7368866328257191\n",
            "recall: 0.2790325164183886\n",
            "f1: 0.4047868014406878\n",
            "\n",
            "Classifier: GNB\n",
            "\n",
            "Training...Done. Took 0.2170276641845703s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.13023638725280762s\n",
            "Metrics:\n",
            "accuracy: 0.8415080183813186\n",
            "balanced_accuracy: 0.5118943879654956\n",
            "tn,fp,fn,tp: 53199,4536,5604,639\n",
            "precision: 0.12347826086956522\n",
            "recall: 0.1023546371936569\n",
            "f1: 0.11192853389385182\n",
            "\n",
            "Classifier: Perceptron\n",
            "\n",
            "Training...Done. Took 0.5541200637817383s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.056181907653808594s\n",
            "Metrics:\n",
            "accuracy: 0.918956516302479\n",
            "balanced_accuracy: 0.7090935622904149\n",
            "tn,fp,fn,tp: 55994,1741,3444,2799\n",
            "precision: 0.6165198237885463\n",
            "recall: 0.44834214320038446\n",
            "f1: 0.5191505146990634\n",
            "\n",
            "Classifier: ET\n",
            "\n",
            "Training...Done. Took 0.6607816219329834s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.0529327392578125s\n",
            "Metrics:\n",
            "accuracy: 0.8535277751727156\n",
            "balanced_accuracy: 0.6335555286717174\n",
            "tn,fp,fn,tp: 52358,5377,3994,2249\n",
            "precision: 0.2949121426698138\n",
            "recall: 0.36024347268941215\n",
            "f1: 0.324320426851251\n",
            "\n",
            "Classifier: DT\n",
            "\n",
            "Training...Done. Took 65.4326024055481s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.04856419563293457s\n",
            "Metrics:\n",
            "accuracy: 0.8615774172371753\n",
            "balanced_accuracy: 0.6709445345219486\n",
            "tn,fp,fn,tp: 52412,5323,3533,2710\n",
            "precision: 0.3373583966139674\n",
            "recall: 0.4340861765176998\n",
            "f1: 0.37965816755393667\n",
            "\n",
            "Classifier: RF\n",
            "\n",
            "Training...Done. Took 290.75683856010437s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 1.6161537170410156s\n",
            "Metrics:\n",
            "accuracy: 0.9208321610553628\n",
            "balanced_accuracy: 0.652989239348434\n",
            "tn,fp,fn,tp: 56914,821,4244,1999\n",
            "precision: 0.7088652482269504\n",
            "recall: 0.320198622457152\n",
            "f1: 0.44113428224649676\n",
            "\n",
            "\n",
            "\n",
            "Embedding: universal-sentence-encoder\n",
            "\n",
            "Processing embedding..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Embedding train set..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [02:51<00:00, 10.75s/it]\n",
            "  0%|          | 0/7 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Embedding test set..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [01:30<00:00, 12.90s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "\n",
            "Classifier: LR\n",
            "\n",
            "Training...Done. Took 9.246548652648926s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.20728802680969238s\n",
            "Metrics:\n",
            "accuracy: 0.9262246397199038\n",
            "balanced_accuracy: 0.8117646422345847\n",
            "tn,fp,fn,tp: 55078,2657,2063,4180\n",
            "precision: 0.611379259909317\n",
            "recall: 0.6695498958833894\n",
            "f1: 0.6391437308868502\n",
            "\n",
            "Classifier: SGD\n",
            "\n",
            "Training...Done. Took 2.063481330871582s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.17361140251159668s\n",
            "Metrics:\n",
            "accuracy: 0.9318515739785551\n",
            "balanced_accuracy: 0.7773104484453088\n",
            "tn,fp,fn,tp: 55964,1771,2589,3654\n",
            "precision: 0.6735483870967742\n",
            "recall: 0.5852955309947141\n",
            "f1: 0.6263284196091875\n",
            "\n",
            "Classifier: Ridge\n",
            "\n",
            "Training...Done. Took 1.4535343647003174s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.13072705268859863s\n",
            "Metrics:\n",
            "accuracy: 0.9317265309950296\n",
            "balanced_accuracy: 0.7393121324722348\n",
            "tn,fp,fn,tp: 56487,1248,3120,3123\n",
            "precision: 0.7144818119423473\n",
            "recall: 0.5002402691013935\n",
            "f1: 0.5884680610514414\n",
            "\n",
            "Classifier: LDA\n",
            "\n",
            "Training...Done. Took 18.418062925338745s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.20890522003173828s\n",
            "Metrics:\n",
            "accuracy: 0.9221294820094408\n",
            "balanced_accuracy: 0.8074242007894776\n",
            "tn,fp,fn,tp: 54845,2890,2092,4151\n",
            "precision: 0.5895469393552052\n",
            "recall: 0.6649046932564472\n",
            "f1: 0.6249623607347184\n",
            "\n",
            "Classifier: PA\n",
            "\n",
            "Training...Done. Took 2.3607335090637207s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.1766188144683838s\n",
            "Metrics:\n",
            "accuracy: 0.9252242958516991\n",
            "balanced_accuracy: 0.7752813761961592\n",
            "tn,fp,fn,tp: 55517,2218,2566,3677\n",
            "precision: 0.6237489397794741\n",
            "recall: 0.588979657216082\n",
            "f1: 0.6058658757620695\n",
            "\n",
            "Classifier: GNB\n",
            "\n",
            "Training...Done. Took 0.8170604705810547s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.5153982639312744s\n",
            "Metrics:\n",
            "accuracy: 0.8703304260839664\n",
            "balanced_accuracy: 0.844867769178695\n",
            "tn,fp,fn,tp: 50605,7130,1166,5077\n",
            "precision: 0.4159089047267961\n",
            "recall: 0.8132308185167387\n",
            "f1: 0.5503523035230352\n",
            "\n",
            "Classifier: Perceptron\n",
            "\n",
            "Training...Done. Took 1.5839519500732422s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.17611002922058105s\n",
            "Metrics:\n",
            "accuracy: 0.9214104848541687\n",
            "balanced_accuracy: 0.7750254387278002\n",
            "tn,fp,fn,tp: 55247,2488,2540,3703\n",
            "precision: 0.5981263123889518\n",
            "recall: 0.5931443216402371\n",
            "f1: 0.5956248994691974\n",
            "\n",
            "Classifier: ET\n",
            "\n",
            "Training...Done. Took 1.556370496749878s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.12550568580627441s\n",
            "Metrics:\n",
            "accuracy: 0.8701897527275001\n",
            "balanced_accuracy: 0.6898593663146424\n",
            "tn,fp,fn,tp: 52765,4970,3335,2908\n",
            "precision: 0.36912922061436915\n",
            "recall: 0.4658016979016498\n",
            "f1: 0.4118688478153105\n",
            "\n",
            "Classifier: DT\n",
            "\n",
            "Training...Done. Took 440.14904165267944s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 0.13486742973327637s\n",
            "Metrics:\n",
            "accuracy: 0.8853512144799774\n",
            "balanced_accuracy: 0.7518318984396845\n",
            "tn,fp,fn,tp: 52985,4750,2585,3658\n",
            "precision: 0.4350618458610847\n",
            "recall: 0.5859362485984302\n",
            "f1: 0.49935158009692165\n",
            "\n",
            "Classifier: RF\n",
            "\n",
            "Training...Done. Took 803.7031197547913s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 2.7303237915039062s\n",
            "Metrics:\n",
            "accuracy: 0.9322579636750132\n",
            "balanced_accuracy: 0.7210349262257126\n",
            "tn,fp,fn,tp: 56781,954,3380,2863\n",
            "precision: 0.750065496463191\n",
            "recall: 0.458593624859843\n",
            "f1: 0.5691848906560636\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}