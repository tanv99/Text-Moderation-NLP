{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "USEL+Ensembled",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNudL5Dxqe3p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e579e779-fa26-48d5-db79-a665defcdcb5"
      },
      "source": [
        "%cd '/content/drive/My Drive/Toxic Content Detection'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Toxic Content Detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL-zTu0lqlUo"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUF85SJTqu9b"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = re.split('\\W+', text)\n",
        "    text = \" \".join(tokens).strip().lower()\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNhQ8m3-qxfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "9b9940fa-de1b-4c2e-9aa0-07fb46044301"
      },
      "source": [
        "train = pd.read_csv('Datasets/jigsaw-toxic-comment/train.csv', index_col=\"id\")\n",
        "train['toxic'] = (train['toxic'] | train['severe_toxic'] | train['obscene'] | train['threat'] | train['insult'] | train['identity_hate']).astype('category')\n",
        "train = train.drop(['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1)\n",
        "train['comment_text'] = train['comment_text'].apply(preprocess)\n",
        "\n",
        "train.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 159571 entries, 0000997932d777bf to fff46fc426af1f9a\n",
            "Data columns (total 2 columns):\n",
            " #   Column        Non-Null Count   Dtype   \n",
            "---  ------        --------------   -----   \n",
            " 0   comment_text  159571 non-null  object  \n",
            " 1   toxic         159571 non-null  category\n",
            "dtypes: category(1), object(1)\n",
            "memory usage: 2.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEKJ7x41rLSg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "4c8d8406-f6e6-4087-fcdf-4c4acefdd02f"
      },
      "source": [
        "test = pd.read_csv('Datasets/jigsaw-toxic-comment/test.csv', index_col=\"id\")\n",
        "test_labels = pd.read_csv('Datasets/jigsaw-toxic-comment/test_labels.csv', index_col=\"id\")\n",
        "test = test.join(test_labels)\n",
        "test['toxic'] = (test['toxic'] | test['severe_toxic'] | test['obscene'] | test['threat'] | test['insult'] | test['identity_hate']).astype('category')\n",
        "test = test.drop(['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1)\n",
        "test = test.drop(test[test['toxic'] == -1].index)\n",
        "test['comment_text'] = test['comment_text'].apply(preprocess)\n",
        "\n",
        "test.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 63978 entries, 0001ea8717f6de06 to fffb5451268fb5ba\n",
            "Data columns (total 2 columns):\n",
            " #   Column        Non-Null Count  Dtype   \n",
            "---  ------        --------------  -----   \n",
            " 0   comment_text  63978 non-null  object  \n",
            " 1   toxic         63978 non-null  category\n",
            "dtypes: category(1), object(1)\n",
            "memory usage: 1.0+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v18XCF1rP6p"
      },
      "source": [
        "x_test_raw, y_test = test['comment_text'], test['toxic']\n",
        "x_train_raw, y_train = train['comment_text'], train['toxic']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy2QTrTCrbQW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "fd63990e-6446-4d4a-87fa-1b66d96f3556"
      },
      "source": [
        "!pip3 install --quiet sentencepiece\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import sentencepiece as spm\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "def use_lite_embed():\n",
        "    module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\")\n",
        "    input_placeholder = tf.sparse_placeholder(tf.int64, shape=[None, None])\n",
        "    encodings = module(\n",
        "        inputs=dict(\n",
        "            values=input_placeholder.values,\n",
        "            indices=input_placeholder.indices,\n",
        "            dense_shape=input_placeholder.dense_shape))\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        spm_path = sess.run(module(signature=\"spm_path\"))\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.Load(spm_path)\n",
        "\n",
        "    def process_to_IDs_in_sparse_format(sp, sentences):\n",
        "        ids = [sp.EncodeAsIds(x) for x in sentences]\n",
        "        max_len = max(len(x) for x in ids)\n",
        "        dense_shape=(len(ids), max_len)\n",
        "        values=[item for sublist in ids for item in sublist]\n",
        "        indices=[[row,col] for row in range(len(ids)) for col in range(len(ids[row]))]\n",
        "        return (values, indices, dense_shape)\n",
        "\n",
        "    def embed(msgs):\n",
        "        values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, msgs)\n",
        "        \n",
        "        with tf.Session() as session:\n",
        "            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "            return session.run(\n",
        "                encodings,\n",
        "                feed_dict={input_placeholder.values: values,\n",
        "                            input_placeholder.indices: indices,\n",
        "                            input_placeholder.dense_shape: dense_shape})\n",
        "    return embed\n",
        "\n",
        "embed = use_lite_embed()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 25.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 6.4MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 9.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 11.4MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 7.1MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 8.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 9.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 9.8MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 9.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 9.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 9.0MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 9.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 9.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 9.0MB/s \n",
            "\u001b[?25hWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bipEawLvTR2W"
      },
      "source": [
        "def batch_map(func, iterable, batch_size=1):\n",
        "    from tqdm import tqdm\n",
        "    l = len(iterable)\n",
        "    result = list()\n",
        "    for ndx in tqdm(range(0, l, batch_size)):\n",
        "        result.extend(func(iterable[ndx:min(ndx + batch_size, l)]))\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pej9eMEHRU6Y"
      },
      "source": [
        "print(\"Embedding train set...\", end=\"\")\n",
        "x_train = batch_map(embed, x_train_raw.to_numpy().reshape((-1)), batch_size=10000)\n",
        "print(\"Done\")\n",
        "\n",
        "print(\"Embedding test set...\", end=\"\")\n",
        "x_test = batch_map(embed, x_test_raw.to_numpy().reshape((-1)), batch_size=10000)\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQULIUC4kF7w"
      },
      "source": [
        "np.savez('Models/Embed+SKLearn/Original/data.npz', x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y6JhH7-s2wq"
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, PassiveAggressiveClassifier\n",
        "\n",
        "classifiers = [(\"LR\", LogisticRegression()),\n",
        "               (\"SGD\", SGDClassifier()),\n",
        "               (\"Ridge\", RidgeClassifier()),\n",
        "               (\"LDA\", LinearDiscriminantAnalysis()),\n",
        "               (\"PA\", PassiveAggressiveClassifier())]\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "clf = VotingClassifier(estimators=classifiers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb3kPecjUYsE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a8a851a3-9954-45b3-b22b-cfb51addf739"
      },
      "source": [
        "# Train with timing\n",
        "print(\"Training...\", end=\"\")\n",
        "time_to_train = time()\n",
        "clf.fit(x_train, y_train)\n",
        "time_to_train = time() - time_to_train\n",
        "print(f\"Done. Took {time_to_train}s\")\n",
        "\n",
        "# Save model\n",
        "print(\"Saving model...\", end=\"\")\n",
        "with open(f\"Models/Embed+SKLearn/Original/USEL-Voting.model\", 'wb') as clf_file:\n",
        "    pickle.dump(clf, clf_file)\n",
        "print(\"Done\")\n",
        "\n",
        "# Predict with timing\n",
        "print(\"Predicting for test set...\", end=\"\")\n",
        "time_to_predict = time()\n",
        "prediction = clf.predict(x_test)\n",
        "time_to_predict = time() - time_to_predict\n",
        "print(f\"Done. Took {time_to_predict}s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...Done. Took 34.23701071739197s\n",
            "Saving model...Done\n",
            "Predicting for test set...Done. Took 1.3672964572906494s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOQKci2Qh1oG"
      },
      "source": [
        "# Metrics\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "def confusion_matrix_proper(y_true, pred):\n",
        "    return \",\".join(str(score) for score in confusion_matrix(y_true, pred).ravel())\n",
        "\n",
        "metrics = [\n",
        "           {\"name\": \"accuracy\", \"metric\": accuracy_score},\n",
        "           {\"name\": \"balanced_accuracy\", \"metric\": balanced_accuracy_score},\n",
        "           {\"name\": \"tn,fp,fn,tp\", \"metric\": confusion_matrix_proper},\n",
        "           {\"name\": \"precision\", \"metric\": precision_score},\n",
        "           {\"name\": \"recall\", \"metric\": recall_score},\n",
        "           {\"name\": \"f1\", \"metric\": f1_score},\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmbq01qkiMNc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "bb415ead-4122-428b-a2c3-75d3dde05e96"
      },
      "source": [
        "print(f\"Metrics:\")\n",
        "with open('Models/Embed+SKLearn/Original/models.csv', 'a') as logfile:\n",
        "    logfile.write(f\"{time()},Voting,USEL,{time_to_train},{time_to_predict}\")\n",
        "    for metric in metrics:\n",
        "        score = metric['metric'](y_test, prediction)\n",
        "        print(f\"{metric['name']}: {score}\")\n",
        "        logfile.write(f\",{score}\")\n",
        "    logfile.write(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metrics:\n",
            "accuracy: 0.9300384507174341\n",
            "balanced_accuracy: 0.8082348192563356\n",
            "tn,fp,fn,tp: 55401,2334,2142,4101\n",
            "precision: 0.6372960372960373\n",
            "recall: 0.6568957232099952\n",
            "f1: 0.6469474680548983\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}